{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Consulting Project\n",
    "\n",
    "## (Henri's Solution + Additional Comments)\n",
    "\n",
    "A technology start-up in California needs your help!\n",
    "\n",
    "It's time for you to go to San Francisco to help out a tech startup!\n",
    "\n",
    "They've been recently hacked and need your help finding out about the hackers!\n",
    "\n",
    "Luckily their forensic engineers have grabbed valuable data about the hacks, including information like:\n",
    "- session\n",
    "- time\n",
    "- locations\n",
    "- wpm typing speed\n",
    "- etc.\n",
    "\n",
    "The forensic engineer relates to you what she has been able to figure out so far.  She has been able to grab meta-data of each session that the hackers used to connect to their servers.\n",
    "\n",
    "These are the features of the data.\n",
    "\n",
    "- ```Session_Connection_Time```:  How long the session lasted in minutes.\n",
    "- ```Bytes_Transferred```:  Number of MB transferred during the session.\n",
    "- ```Kali_Trace_Used```:  Indicates if the hacker was using Kali Linux.\n",
    "- ```Servers_Corrupted```:  The number of server corrupted during the attack.\n",
    "- ```Pages_Corrupted```:  The number of pages illegally accessed.\n",
    "- ```Location```:  Location the attack came from (Probably useless because the hackers used VPNs).\n",
    "- ```WPM_Typing_Speed```:  Their estimated typing speed based on session logs.\n",
    "\n",
    "Kali Linux is a Debian-derived Linux distribution designed for digital forensics and penetration testing.  It is maintained and funded by Offensive Security Ltd.  It was developed by Mati Aharoni and Devon Kearns of Offensive Security through the rewrite of BackTrack, their previous information security testing Linux distribution based on Knoppix.  The third core developer, Raphael Hertzog, joined them as a Debian expert.\n",
    "\n",
    "The technology firm has 3 potential hackers that perpetrated the attack.\n",
    "\n",
    "They are certain of the first two hackers, but they aren't very sure if the third hacker was invovled or not.\n",
    "\n",
    "They have rerquested your help!\n",
    "\n",
    "Can you help figure out whether or not the third suspect had anything to do with the attacks, or was it just 2 hackers?\n",
    "\n",
    "It is probably not possible to know for sure, but maybe what you've just learned about clustering can help.\n",
    "\n",
    "One last key fact, the forensic engineer knows that the hackers trade off attacks.\n",
    "\n",
    "Meaning they should each have roughly the same amount of attacks.\n",
    "\n",
    "For example, if there were 100 total attacks, then in a 2 hacker situation each should have about 50 hacks, and in a 3 hacker situation each would have about 33 hacks.\n",
    "\n",
    "The engineer believes this is the key element to solving this, but does not know how to distinguish this unlabeled data into groups of hackers.\n",
    "\n",
    "Best of luck with this project, it should be a fun one!\n",
    "\n",
    "If you get stuck, feel free to go straight to the solution lecture.\n",
    "\n",
    "Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark Session:\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"hack\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"hack_data.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Session_Connection_Time: double (nullable = true)\n",
      " |-- Bytes Transferred: double (nullable = true)\n",
      " |-- Kali_Trace_Used: integer (nullable = true)\n",
      " |-- Servers_Corrupted: double (nullable = true)\n",
      " |-- Pages_Corrupted: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- WPM_Typing_Speed: double (nullable = true)\n",
      "\n",
      "\n",
      "\n",
      "Row(Session_Connection_Time=8.0, Bytes Transferred=391.09, Kali_Trace_Used=1, Servers_Corrupted=2.96, Pages_Corrupted=7.0, Location='Slovenia', WPM_Typing_Speed=72.37)\n",
      "\n",
      "\n",
      "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
      "|summary|Session_Connection_Time| Bytes Transferred|   Kali_Trace_Used|Servers_Corrupted|   Pages_Corrupted|   Location|  WPM_Typing_Speed|\n",
      "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
      "|  count|                    334|               334|               334|              334|               334|        334|               334|\n",
      "|   mean|     30.008982035928145| 607.2452694610777|0.5119760479041916|5.258502994011977|10.838323353293413|       null|57.342395209580864|\n",
      "| stddev|     14.088200614636158|286.33593163576757|0.5006065264451406| 2.30190693339697|  3.06352633036022|       null| 13.41106336843464|\n",
      "|    min|                    1.0|              10.0|                 0|              1.0|               6.0|Afghanistan|              40.0|\n",
      "|    max|                   60.0|            1330.5|                 1|             10.0|              15.0|   Zimbabwe|              75.0|\n",
      "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
      "\n",
      "\n",
      "\n",
      "334\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()\n",
    "print(\"\\n\")\n",
    "\n",
    "print(data.head(1)[0])\n",
    "print(\"\\n\")\n",
    "\n",
    "data.describe().show()\n",
    "print(\"\\n\")\n",
    "\n",
    "print(data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Session_Connection_Time',\n",
       " 'Bytes Transferred',\n",
       " 'Kali_Trace_Used',\n",
       " 'Servers_Corrupted',\n",
       " 'Pages_Corrupted',\n",
       " 'Location',\n",
       " 'WPM_Typing_Speed']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['Session_Connection_Time', 'Bytes Transferred',\n",
    "                                       'Kali_Trace_Used', 'Servers_Corrupted',\n",
    "                                       'Pages_Corrupted', 'WPM_Typing_Speed'],\n",
    "                            outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_assembled = assembler.transform(dataset=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Session_Connection_Time: double (nullable = true)\n",
      " |-- Bytes Transferred: double (nullable = true)\n",
      " |-- Kali_Trace_Used: integer (nullable = true)\n",
      " |-- Servers_Corrupted: double (nullable = true)\n",
      " |-- Pages_Corrupted: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- WPM_Typing_Speed: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_assembled.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_model = std_scaler.fit(dataset=data_assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled = scaler_model.transform(dataset=data_assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Session_Connection_Time=8.0, Bytes Transferred=391.09, Kali_Trace_Used=1, Servers_Corrupted=2.96, Pages_Corrupted=7.0, Location='Slovenia', WPM_Typing_Speed=72.37, features=DenseVector([8.0, 391.09, 1.0, 2.96, 7.0, 72.37]), scaled_features=DenseVector([0.5679, 1.3658, 1.9976, 1.2859, 2.2849, 5.3963]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_scaled.head(1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_2 = KMeans(featuresCol=\"scaled_features\", k=2)\n",
    "kmeans_3 = KMeans(featuresCol=\"scaled_features\", k=3)\n",
    "kmeans_4 = KMeans(featuresCol=\"scaled_features\", k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_fitted_2 = kmeans_2.fit(data_scaled)\n",
    "kmeans_fitted_3 = kmeans_3.fit(data_scaled)\n",
    "kmeans_fitted_4 = kmeans_4.fit(data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric(2) = 601.8.\n",
      "metric(3) = 434.1.\n",
      "metric(4) = 267.1.\n"
     ]
    }
   ],
   "source": [
    "print(\"metric(2) = {:.1f}.\".format(kmeans_fitted_2.computeCost(data_scaled)))\n",
    "print(\"metric(3) = {:.1f}.\".format(kmeans_fitted_3.computeCost(data_scaled)))\n",
    "print(\"metric(4) = {:.1f}.\".format(kmeans_fitted_4.computeCost(data_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the groupings that were made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = kmeans_fitted_2.transform(data_scaled).select([\"prediction\"])\n",
    "df_3 = kmeans_fitted_3.transform(data_scaled).select([\"prediction\"])\n",
    "df_4 = kmeans_fitted_4.transform(data_scaled).select([\"prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|  167|\n",
      "|         0|  167|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|   83|\n",
      "|         2|   84|\n",
      "|         0|  167|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|   83|\n",
      "|         3|   84|\n",
      "|         2|   79|\n",
      "|         0|   88|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.groupBy(\"prediction\").count().show()\n",
    "df_3.groupBy(\"prediction\").count().show()\n",
    "df_4.groupBy(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***There appears to be 2 hackers involved.  Notice how the counts are identical for the kmeans model where k=2.  The counts are most certainly not equal for the case where k=3.***\n",
    "\n",
    "***The third suspect had nothing to do with the attacks.  Only the 2 initially suspected hackers were involved.***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
