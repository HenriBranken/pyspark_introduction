{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://changhsinlee.com/pyspark-udf/\n",
    "# How to Turn Python Functions into PySpark Functions (UDF)\n",
    "\n",
    "Here’s the problem: I have a Python function that iterates over my data, but going through each row in the dataframe takes several days. If I have a computing cluster with many nodes, how can I distribute this Python function in PySpark to speed up this process — maybe cut the total time down to less than a few hours — with the least amount of work?\n",
    "\n",
    "In other words, how do I turn a Python function into a Spark user defined function, or _UDF_? I’ll explain my solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registering a UDF\n",
    "\n",
    "PySpark UDFs work in a similar way as the pandas ```.map()``` and ```.apply()``` methods for pandas series and dataframes. If I have a function that can use values from a row in the dataframe as input, then I can map it to the entire dataframe. The only difference is that with PySpark UDFs I have to specify the output data type.\n",
    "\n",
    "As an example, I will create a PySpark dataframe from a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"example-from-chinese-dude\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- integers: long (nullable = true)\n",
      " |-- floats: double (nullable = true)\n",
      " |-- integer_arrays: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n",
      "\n",
      "\n",
      "+--------+------+--------------+\n",
      "|integers|floats|integer_arrays|\n",
      "+--------+------+--------------+\n",
      "|       1|  -1.0|        [1, 2]|\n",
      "|       2|   0.5|     [3, 4, 5]|\n",
      "|       3|   2.7|  [6, 7, 8, 9]|\n",
      "+--------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example Data:\n",
    "df_pd = pd.DataFrame(data={'integers': [1, 2, 3],\n",
    "                           'floats': [-1.0, 0.5, 2.7],\n",
    "                           'integer_arrays': [[1, 2], [3, 4, 5], [6, 7, 8, 9]]})\n",
    "\n",
    "df = spark.createDataFrame(df_pd)\n",
    "df.printSchema()\n",
    "print(\"\\n\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primitive type outputs\n",
    "\n",
    "Let's say I have a python function ```square()``` that squares a number, and I want to register this functions as a Spark UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(value):\n",
    "    return value**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as the python function's output has a corresponding data type in Spark, then I can turn it into a UDF.  When registering UDFs, I have to specify the data type using the types from ```pyspark.sql.types```.  All the types supported by PySpark can be found [here](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=types#module-pyspark.sql.types).\n",
    "\n",
    "**Here's a small gotcha**---because Spark UDF does not convert integers to floats, unlike the Python function which works for both integers and floats, a Spark UDF will return a column of NULLS if the input data type does not match the output data type, as in the following example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering UDF with integer type output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integer type output.\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_udf_int = udf(f=lambda z: square(z), returnType=IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+-------------+\n",
      "|integers|floats|int_squared|float_squared|\n",
      "+--------+------+-----------+-------------+\n",
      "|       1|  -1.0|          1|         null|\n",
      "|       2|   0.5|          4|         null|\n",
      "|       3|   2.7|          9|         null|\n",
      "+--------+------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"integers\",\n",
    "          \"floats\",\n",
    "          square_udf_int(\"integers\").alias(\"int_squared\"),\n",
    "          square_udf_int(\"floats\").alias(\"float_squared\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering UDF with float type output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Float type output.\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_udf_float = udf(f=lambda z: square(z), returnType=FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+-------------+\n",
      "|integers|floats|int_squared|float_squared|\n",
      "+--------+------+-----------+-------------+\n",
      "|       1|  -1.0|       null|          1.0|\n",
      "|       2|   0.5|       null|         0.25|\n",
      "|       3|   2.7|       null|         7.29|\n",
      "+--------+------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"integers\",\n",
    "          \"floats\",\n",
    "          square_udf_float(\"integers\").alias(\"int_squared\"),\n",
    "          square_udf_float(\"floats\").alias(\"float_squared\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the float type output in the Python function\n",
    "\n",
    "Specifying the data type in the Pytyhon function output is probably the safer way.  Because I _(the Chinese guy)_ usually load data into Spark from from Hive tables whose schemas were made by others, specifying the return data type means the UDF should still work as intended even if the Hive schema has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forcing the output to be float\n",
    "def square_float(x):\n",
    "    return float(x**2)\n",
    "\n",
    "\n",
    "square_udf_float2 = udf(f=lambda z: square_float(z), returnType=FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+-------------+\n",
      "|integers|floats|int_squared|float_squared|\n",
      "+--------+------+-----------+-------------+\n",
      "|       1|  -1.0|        1.0|          1.0|\n",
      "|       2|   0.5|        4.0|         0.25|\n",
      "|       3|   2.7|        9.0|         7.29|\n",
      "+--------+------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"integers\",\n",
    "          \"floats\",\n",
    "          square_udf_float2(\"integers\").alias(\"int_squared\"),\n",
    "          square_udf_float2(\"floats\").alias(\"float_squared\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composite type outputs\n",
    "\n",
    "If the output of the Python function is a liast, then the values in the list have to be of the same type, which is specified within ```ArrayType()``` when registering the UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_list(some_iterable):\n",
    "    return [float(val)**2 for val in some_iterable]\n",
    "\n",
    "square_list_udf = udf(f=lambda y: square_list(y), returnType=ArrayType())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
