{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://docs.databricks.com/spark/latest/spark-sql/udf-python.html\n",
    "\n",
    "**The following contains Python UDF examples.**\n",
    "\n",
    "It shows how to:\n",
    "1. register UDFs,\n",
    "2. invoke UDFs, and\n",
    "3. caveats regarding evaluation order of subexpressions in Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"udfs-yippee\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to register a function as a UDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared(value):\n",
    "    return value * value\n",
    "\n",
    "\n",
    "_ = spark.udf.register(name=\"squared_with_Python\", f=squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## According to the ```docstring``` of ```spark.udf.register```:\n",
    "\n",
    "```spark.udf.register(name, f, returnType=None)```  \n",
    "**Register a Python function (including lambda function) or a user-defined function\n",
    "as a SQL function.**\n",
    "- ```name```:  name of the uder-defined function in SQL statements.\n",
    "- ```f```: a Python function, or a user-defined function.  The udf can be either row-at-a-time or vectorized.  See ```pyspark.sql.functions.udf``` and ```pyspark.sql.functions.pandas_udf```.\n",
    "- ```returnType```: The return type of the registered user-defined function.  The value can be either a ```pyspark.sql.types.DataType``` object or a DDL-formatted type string.\n",
    "\n",
    "**```return```**:  a user-defined function.\n",
    "\n",
    "```returnType``` can be optionally specified when ```f``` is a Python function, but **NOT** when ```f``` is a user-defined function.\n",
    "\n",
    "### 1.  When ```f``` is a ```Python``` function:\n",
    "```returnType``` defaults to string type and can be optionally specified.  The produced object must match the specified type.  In this case, this API works as if ```register(name, f, returnType=StringType())```.\n",
    "\n",
    "```python\n",
    ">>> strlen = spark.udf.register(\"stringLength\", lambda x: len(x))  \n",
    ">>> spark.sql(\"SELECT strinLength('test')\").collect()  \n",
    "[Row(stringLength(test)='4')]\n",
    "\n",
    ">>> spark.sql(\"SELECT 'foo' AS text\").select(strlen(\"text\")).collect()  \n",
    "[Row(stringLengthString(text)='3')]\n",
    "\n",
    ">>> from pyspark.sql.types import IntegerType  \n",
    ">>> _ = spark.udf.register(name=\"stringLengthInt\", f=lambda x: len(x), returnType=IntegerType())  # Notice that we specify returnType\n",
    ">>> spark.sql(\"SELECT stringLengthInt('test')\").collect()  \n",
    "[Row(stringLengthInt(test)=4)]```\n",
    "\n",
    "### 2.  When ```f``` is a user-defined function:\n",
    "\n",
    "Spark uses the return type of the given user-defined function as the return type of the registered user-defined function.  ```returnType``` should **not be specified.**  In this case, this API works as if ```register(name, f)```.\n",
    "\n",
    "```python\n",
    ">>> from pyspark.sql types import IntegerType  \n",
    ">>> from pyspark.sql.functions import udf  \n",
    ">>> slen = udf(lambda s: len(s), IntegerType())  \n",
    ">>> _ = spark.udf.register(name=\"slen\", f=slen)  # Notice that we are not specifying returnType.  \n",
    ">>> spark.sql(\"SELECT slen('test')\").collect()  \n",
    "[Row(slen(test)=4)]\n",
    "\n",
    ">>> import random  \n",
    ">>> from pyspark.sql functions import udf  \n",
    ">>> from pyspark.sql.types import IntegerType  \n",
    ">>> random_udf = udf(lambda: random.randint(0, 100), IntegerType()).asNondeterministic()  \n",
    ">>> spark.sql(\"SELECT random_udf()\").collect()  \n",
    "[Row(random_udf()=82)]\n",
    "\n",
    ">>> from pyspark.sql.function imort pandas_udf, PandasUDFType  \n",
    ">>> @pandas_udf(\"integer\", PandasUDFType.SCALAR)  \n",
    "... def add_one(x):\n",
    "...     return x + 1\n",
    "\n",
    ">>> _ = spark.udf.register(\"add_one\", add_one)\n",
    ">>> spark.sql(\"SELECT add_one(id) FROM range(3)\").collect()  \n",
    "[Row(add_one(id)=1), Row(add_one(id)=2), Row(add_one(id)=3)]\n",
    "\n",
    ">>> @pandas_udf(\"integer\", PandasUDFType.GROUPED_AGG)\n",
    "... def sum_udf(v):\n",
    "        return v.sum()\n",
    "\n",
    ">>> _ = spark.udf.register(\"sum_udf\", sum_udf)  \n",
    ">>> q = \"SELECT sum_udf(v1) FROM VALUES (3, 0), (2, 0), (1, 1) tbl(v1, v2) GROUP BY v2\"  \n",
    ">>> spark.sql(q).collect()  \n",
    "[Row(sum_udf(v1)=1), Row(sum_udf(v1)=5)]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally set the return type of your UDF.  The **default** return type is ```StringType```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "# pyspark.sql.types.LongType is a Long data type, i.e., a signed 64-bit integer.\n",
    "# If the values are beyond the range of [-9223372036854775808, 9223372036854775807], please use DecimalType.\n",
    "\n",
    "\n",
    "def squared_typed(value):\n",
    "    return value * value\n",
    "\n",
    "\n",
    "_ = spark.udf.register(name=\"squared_with_Python\", f=squared_typed, returnType=LongType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call the UDF in Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1, 20).registerTempTable(\"test\")\n",
    "# Registers this DataFrame as a temporary table using the given name.\n",
    "# The lifetime of this temporary table is tied to the SparkSession that was used to create this DataFrame.\n",
    "\n",
    "df = spark.sql(\"SELECT * FROM test\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|id_squared|\n",
      "+---+----------+\n",
      "|  1|         1|\n",
      "|  2|         4|\n",
      "|  3|         9|\n",
      "|  4|        16|\n",
      "|  5|        25|\n",
      "|  6|        36|\n",
      "|  7|        49|\n",
      "|  8|        64|\n",
      "|  9|        81|\n",
      "| 10|       100|\n",
      "| 11|       121|\n",
      "| 12|       144|\n",
      "| 13|       169|\n",
      "| 14|       196|\n",
      "| 15|       225|\n",
      "| 16|       256|\n",
      "| 17|       289|\n",
      "| 18|       324|\n",
      "| 19|       361|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.sql(\"SELECT id, squared_with_Python(id) AS id_squared FROM test\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use ```UDF``` with DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [this link](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=udf#pyspark.sql.functions.udf):\n",
    "\n",
    "```pyspark.sql.functions.udf(f=None, returnType=StringType)```\n",
    "\n",
    "Creates a user-defined function (UDF).\n",
    "- ```f```: python function if used as a standalone function.\n",
    "- ```returnType```: the return type of the user-defined function.  The value can be either a ```pyspark.sql.types.DataType``` object or a DDL-formatted type string.\n",
    "\n",
    "**Note**:  The user-defined functions are considered deterministic by default.  Due to optimization, duplicate invocations may be eliminated or the function may even be invoked more times than it is present in the query.  If your function is **not deterministic**, call ```asNondeterministic``` on the user defined function.  E.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import random\n",
    "\n",
    "random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Moving on to the DataBricks example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "squared_udf = udf(squared, LongType())\n",
    "df = spark.table(\"test\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|id_squared|\n",
      "+---+----------+\n",
      "|  1|         1|\n",
      "|  2|         4|\n",
      "|  3|         9|\n",
      "|  4|        16|\n",
      "|  5|        25|\n",
      "|  6|        36|\n",
      "|  7|        49|\n",
      "|  8|        64|\n",
      "|  9|        81|\n",
      "| 10|       100|\n",
      "| 11|       121|\n",
      "| 12|       144|\n",
      "| 13|       169|\n",
      "| 14|       196|\n",
      "| 15|       225|\n",
      "| 16|       256|\n",
      "| 17|       289|\n",
      "| 18|       324|\n",
      "| 19|       361|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\", squared_udf(\"id\").alias(\"id_squared\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can declare the same UDF using **annotation syntax:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(\"long\")\n",
    "def squared_udf(value):\n",
    "    return value * value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|id_squared|\n",
      "+---+----------+\n",
      "|  1|         1|\n",
      "|  2|         4|\n",
      "|  3|         9|\n",
      "|  4|        16|\n",
      "|  5|        25|\n",
      "|  6|        36|\n",
      "|  7|        49|\n",
      "|  8|        64|\n",
      "|  9|        81|\n",
      "| 10|       100|\n",
      "| 11|       121|\n",
      "| 12|       144|\n",
      "| 13|       169|\n",
      "| 14|       196|\n",
      "| 15|       225|\n",
      "| 16|       256|\n",
      "| 17|       289|\n",
      "| 18|       324|\n",
      "| 19|       361|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.table(\"test\")\n",
    "df.select(\"id\", squared_udf(\"id\").alias(\"id_squared\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation order and Null Checking.\n",
    "\n",
    "Spark SQL does not guarantee the order of evaluation of subexpressions.  In particular, the inputs of an operator or function are not necessarily evaluated left-to-right or in any other fixed order.  For example, logical ```AND``` and ```OR``` expressions do not have left-to-right \"short-circuiting\" semantics.\n",
    "\n",
    "Therefore, it iks dangerous to rely on the side effects or order of evaluation of Boolean expressions, and the order of ```WHERE``` and ```HAVING``` clauses, since such expressions and clauses can be reordered during query optimization and planning.  Specifically, if a UDF relies on short-circuiting semantics in SQL for null checking, there's no guarantee that the null check will happen before invoking the UDF.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = spark.udf.register(name=\"strlen\", f=lambda s: len(s), returnType=\"int\")\n",
    "# spark.sql(\"SELECT s FROM test1 WHERE s is not null and strlen(s) > 1\")  # no guarantee!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above ```WHERE``` clause does not guarantee the ```strlen``` UDF to be invoked after filtering out nulls.\n",
    "\n",
    "To perform **proper null checking**, we recommend that you do either of the following:\n",
    "- Make the ```UDF``` itself null-aware and **do null checking inside the UDF itself.**\n",
    "- Use ```IF``` or ```CASE WHEN``` expressions to do the null check and invoke the UDF in a conditional branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = spark.udf.register(name=\"strlen_nullsafe\", f=lambda s: len(s) if not s is False else -1, returnType=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"SELECT s FROM test1 WHERE s is not null and strlen_nullsafe(s) > 1\") --> OK, good to go.\n",
    "# spark.sql(\"SELECT s from test1 WHERE if(s is not null, strlen(s), null) > 1\") --> OK, good to go."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
