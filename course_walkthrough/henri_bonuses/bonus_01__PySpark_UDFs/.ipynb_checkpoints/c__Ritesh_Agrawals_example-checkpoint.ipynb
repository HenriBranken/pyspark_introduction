{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://ragrawal.wordpress.com/2015/10/02/spark-custom-udf-example/\n",
    "\n",
    "## Spark: Custom UDF Example.\n",
    "\n",
    "UDF (User Defined Functions) and UDAF (User Defined Aggregate Functions) are key components of big data languages such as Pig and Hive.  They allow to extend the language constructs to do adhoc processing on distributed datasets.  In this walkthrough, we will focus on writing a custom UDF in Spark.\n",
    "\n",
    "As a motivating example, assume that we are given some student data containing the student's name, subject, and score.  We want to convert the numerical score into ordinal categories based on the following logic:\n",
    "\n",
    "- A:  score >= 80.\n",
    "- B:  60 <= score < 80.\n",
    "- C:  35 <= score < 60.\n",
    "- D:  score < 35.\n",
    "\n",
    "Below is the relevant Python code if you are using PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1:  Basic Python Stuff.  We are generating a random dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Random Data\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('John', 'Math', 17)\n",
      "('John', 'Science', 72)\n",
      "('John', 'Geography', 97)\n",
      "('John', 'History', 8)\n",
      "('Mike', 'Math', 32)\n",
      "('Mike', 'Science', 15)\n",
      "('Mike', 'Geography', 63)\n",
      "('Mike', 'History', 97)\n",
      "('Matt', 'Math', 57)\n",
      "('Matt', 'Science', 60)\n",
      "('Matt', 'Geography', 83)\n",
      "('Matt', 'History', 48)\n"
     ]
    }
   ],
   "source": [
    "students = [\"John\", \"Mike\", \"Matt\"]\n",
    "subjects = [\"Math\", \"Science\", \"Geography\", \"History\"]\n",
    "random.seed(1)\n",
    "data = []\n",
    "\n",
    "for student, subject in itertools.product(students, subjects):\n",
    "    data.append((student, subject, random.randint(0, 100)))\n",
    "\n",
    "for record in data:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2:  Constructing our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Schema Object.\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "schema = StructType([\n",
    "    StructField(\"student\", StringType(), nullable=False),\n",
    "    StructField(\"subject\", StringType(), nullable=False),\n",
    "    StructField(\"score\", IntegerType(), nullable=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   student    subject  score\n",
      "0     John       Math     17\n",
      "1     John    Science     72\n",
      "2     John  Geography     97\n",
      "3     John    History      8\n",
      "4     Mike       Math     32\n",
      "5     Mike    Science     15\n",
      "6     Mike  Geography     63\n",
      "7     Mike    History     97\n",
      "8     Matt       Math     57\n",
      "9     Matt    Science     60\n",
      "10    Matt  Geography     83\n",
      "11    Matt    History     48\n"
     ]
    }
   ],
   "source": [
    "pd_df = pd.DataFrame(data, columns=[\"student\", \"subject\", \"score\"])\n",
    "print(pd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark Session.\n",
    "spark = SparkSession.builder.appName(\"example_c\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----+\n",
      "|student|  subject|score|\n",
      "+-------+---------+-----+\n",
      "|   John|     Math|   17|\n",
      "|   John|  Science|   72|\n",
      "|   John|Geography|   97|\n",
      "|   John|  History|    8|\n",
      "|   Mike|     Math|   32|\n",
      "|   Mike|  Science|   15|\n",
      "|   Mike|Geography|   63|\n",
      "|   Mike|  History|   97|\n",
      "|   Matt|     Math|   57|\n",
      "|   Matt|  Science|   60|\n",
      "|   Matt|Geography|   83|\n",
      "|   Matt|  History|   48|\n",
      "+-------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = spark.createDataFrame(data=pd_df, schema=schema)\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3:  The main part of the code dealing with defining the UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the UDF.\n",
    "def scoreToCategory(score):\n",
    "    if score >= 80:\n",
    "        return \"A\"\n",
    "    elif 60 <= score < 80:\n",
    "        return \"B\"\n",
    "    elif 35 <= score < 60:\n",
    "        return \"C\"\n",
    "    else:  # i.e. score < 35\n",
    "        return \"D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "udfScoreToCategory = udf(f=scoreToCategory, returnType=StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----+--------+\n",
      "|student|  subject|score|category|\n",
      "+-------+---------+-----+--------+\n",
      "|   John|     Math|   17|       D|\n",
      "|   John|  Science|   72|       B|\n",
      "|   John|Geography|   97|       A|\n",
      "|   John|  History|    8|       D|\n",
      "|   Mike|     Math|   32|       D|\n",
      "|   Mike|  Science|   15|       D|\n",
      "|   Mike|Geography|   63|       B|\n",
      "|   Mike|  History|   97|       A|\n",
      "|   Matt|     Math|   57|       C|\n",
      "|   Matt|  Science|   60|       B|\n",
      "|   Matt|Geography|   83|       A|\n",
      "|   Matt|  History|   48|       C|\n",
      "+-------+---------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.withColumn(\"category\", udfScoreToCategory(\"score\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the flip side of the coin, it is important to realise that you should only make use of UDF when absolutely necessary.\n",
    "\n",
    "Click on the following link which redirects you to a ~5-minute youtube clip.\n",
    "\n",
    "[Avoid UDF](https://www.youtube.com/watch?v=SHiZzB4n46A)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
