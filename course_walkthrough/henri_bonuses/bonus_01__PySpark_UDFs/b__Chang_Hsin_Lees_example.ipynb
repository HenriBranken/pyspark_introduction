{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://changhsinlee.com/pyspark-udf/\n",
    "# How to Turn Python Functions into PySpark Functions (UDF)\n",
    "\n",
    "Here’s the problem: I have a Python function that iterates over my data, but going through each row in the dataframe takes several days. If I have a computing cluster with many nodes, how can I distribute this Python function in PySpark to speed up this process — maybe cut the total time down to less than a few hours — with the least amount of work?\n",
    "\n",
    "In other words, how do I turn a Python function into a Spark user defined function, or _UDF_? I’ll explain my solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registering a UDF\n",
    "\n",
    "PySpark UDFs work in a similar way as the pandas ```.map()``` and ```.apply()``` methods for pandas series and dataframes. If I have a function that can use values from a row in the dataframe as input, then I can map it to the entire dataframe. The only difference is that with PySpark UDFs I have to specify the output data type.\n",
    "\n",
    "As an example, I will create a PySpark dataframe from a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"example-from-chinese-dude\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- integers: long (nullable = true)\n",
      " |-- floats: double (nullable = true)\n",
      " |-- integer_arrays: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n",
      "\n",
      "\n",
      "+--------+------+--------------+\n",
      "|integers|floats|integer_arrays|\n",
      "+--------+------+--------------+\n",
      "|       1|  -1.0|        [1, 2]|\n",
      "|       2|   0.5|     [3, 4, 5]|\n",
      "|       3|   2.7|  [6, 7, 8, 9]|\n",
      "+--------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example Data:\n",
    "df_pd = pd.DataFrame(data={'integers': [1, 2, 3],\n",
    "                           'floats': [-1.0, 0.5, 2.7],\n",
    "                           'integer_arrays': [[1, 2], [3, 4, 5], [6, 7, 8, 9]]})\n",
    "\n",
    "df = spark.createDataFrame(df_pd)\n",
    "df.printSchema()\n",
    "print(\"\\n\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primitive type outputs\n",
    "\n",
    "Let's say I have a python function ```square()``` that squares a number, and I want to register this functions as a Spark UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(value):\n",
    "    return value**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as the python function's output has a corresponding data type in Spark, then I can turn it into a UDF.  When registering UDFs, I have to specify the data type using the types from ```pyspark.sql.types```.  All the types supported by PySpark can be found [here](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=types#module-pyspark.sql.types).\n",
    "\n",
    "**Here's a small gotcha**---because Spark UDF does not convert integers to floats, unlike the Python function which works for both integers and floats, a Spark UDF will return a column of NULLS if the input data type does not match the output data type, as in the following example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering UDF with integer type output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integer type output.\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_udf_int = udf(f=lambda z: square(z), returnType=IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+-------------+\n",
      "|integers|floats|int_squared|float_squared|\n",
      "+--------+------+-----------+-------------+\n",
      "|       1|  -1.0|          1|         null|\n",
      "|       2|   0.5|          4|         null|\n",
      "|       3|   2.7|          9|         null|\n",
      "+--------+------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"integers\",\n",
    "          \"floats\",\n",
    "          square_udf_int(\"integers\").alias(\"int_squared\"),\n",
    "          square_udf_int(\"floats\").alias(\"float_squared\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering UDF with float type output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Float type output.\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_udf_float = udf(f=lambda z: square(z), returnType=FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+-------------+\n",
      "|integers|floats|int_squared|float_squared|\n",
      "+--------+------+-----------+-------------+\n",
      "|       1|  -1.0|       null|          1.0|\n",
      "|       2|   0.5|       null|         0.25|\n",
      "|       3|   2.7|       null|         7.29|\n",
      "+--------+------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"integers\",\n",
    "          \"floats\",\n",
    "          square_udf_float(\"integers\").alias(\"int_squared\"),\n",
    "          square_udf_float(\"floats\").alias(\"float_squared\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the float type output in the Python function\n",
    "\n",
    "Specifying the data type in the Pytyhon function output is probably the safer way.  Because I _(the Chinese guy)_ usually load data into Spark from from Hive tables whose schemas were made by others, specifying the return data type means the UDF should still work as intended even if the Hive schema has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forcing the output to be float\n",
    "def square_float(x):\n",
    "    return float(x**2)\n",
    "\n",
    "\n",
    "square_udf_float2 = udf(f=lambda z: square_float(z), returnType=FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+-------------+\n",
      "|integers|floats|int_squared|float_squared|\n",
      "+--------+------+-----------+-------------+\n",
      "|       1|  -1.0|        1.0|          1.0|\n",
      "|       2|   0.5|        4.0|         0.25|\n",
      "|       3|   2.7|        9.0|         7.29|\n",
      "+--------+------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"integers\",\n",
    "          \"floats\",\n",
    "          square_udf_float2(\"integers\").alias(\"int_squared\"),\n",
    "          square_udf_float2(\"floats\").alias(\"float_squared\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composite type outputs\n",
    "\n",
    "If the output of the Python function is a liast, then the values in the list have to be of the same type, which is specified within ```ArrayType()``` when registering the UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------------+\n",
      "|integer_arrays|squares_of_integer_arrays|\n",
      "+--------------+-------------------------+\n",
      "|[1, 2]        |[1.0, 4.0]               |\n",
      "|[3, 4, 5]     |[9.0, 16.0, 25.0]        |\n",
      "|[6, 7, 8, 9]  |[36.0, 49.0, 64.0, 81.0] |\n",
      "+--------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def square_list(some_iterable):\n",
    "    return [float(val)**2 for val in some_iterable]\n",
    "\n",
    "\n",
    "square_list_udf = udf(f=lambda y: square_list(y), returnType=ArrayType(FloatType()))\n",
    "\n",
    "df.select(\"integer_arrays\", \n",
    "          square_list_udf(\"integer_arrays\").alias(\"squares_of_integer_arrays\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a function that returns a tuple of mixed typed values, I can make a corresponding ```StructType()```, which is a composite type in Spark, and specify what is in the struct with ```StructField()```.  For example, if I have a function that returns the position and the letter from ```ascii_letters```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from pyspark.sql.types import StructType, StructField, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 'b']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_ascii(number):\n",
    "    return [number, string.ascii_letters[number]]\n",
    "\n",
    "\n",
    "convert_ascii(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_schema = StructType(fields=[\n",
    "    StructField(name=\"number\", dataType=IntegerType(), nullable=False),\n",
    "    StructField(name=\"letters\", dataType=StringType(), nullable=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|integers|ascii_mapping|\n",
      "+--------+-------------+\n",
      "|       1|       [1, b]|\n",
      "|       2|       [2, c]|\n",
      "|       3|       [3, d]|\n",
      "+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_convert_ascii = udf(f=lambda z: convert_ascii(z), returnType=array_schema)\n",
    "\n",
    "df_ascii = df.select(\"integers\", spark_convert_ascii(\"integers\").alias(\"ascii_mapping\"))\n",
    "df_ascii.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the schema looks like a tree, with nullable option specified as in ```StructField()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- integers: long (nullable = true)\n",
      " |-- ascii_mapping: struct (nullable = true)\n",
      " |    |-- number: integer (nullable = false)\n",
      " |    |-- letters: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ascii.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some UDF problems I've _(the Chinese guy)_ seen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Py4JJaveError\n",
    "\n",
    "Most of the ```Py4JJaveError``` exceptions I've seen came from mismatched data types between Python and Spark, especially when the function uses a data type from a Python module like ```numpy```.  So I'd first look into that if there's an error.\n",
    "\n",
    "For example, if the output is a ```numpy.ndarray```, then the UDF thows an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|int_arrays|\n",
      "+----------+\n",
      "| [1, 2, 3]|\n",
      "|    [4, 5]|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "pd_np = pd.DataFrame(data={\"int_arrays\": [[1, 2, 3], [4, 5]]})\n",
    "df_np = spark.createDataFrame(pd_np)\n",
    "df_np.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 9]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# squares with a numpy function, which returns a np.ndarray\n",
    "def square_array_wrong(x):\n",
    "    return np.square(x)\n",
    "\n",
    "\n",
    "my_result = square_array_wrong([1, 2, 3])\n",
    "print(my_result)\n",
    "print(type(my_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns a ```numpy.ndarray``` whose values are also numpy objects ```numpy.int32``` instead of Python primitives.\n",
    "\n",
    "When combined with the spark udf function, it throws a ```Py4JJavaError```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following code to see the Py4JJavaError that would be produced.\n",
    "# spark_square_array_wrong = udf(f=square_array_wrong, returnType=ArrayType(FloatType()))\n",
    "\n",
    "# df_np.withColumn(\"doubled\", spark_square_array_wrong(\"int_arrays\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is to convert it back to a list whose values are Python primitives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_array_right(x):\n",
    "    return np.square(x).tolist()\n",
    "\n",
    "spark_square_array_right = udf(f=square_array_right, returnType=ArrayType(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the UDF will work as intended:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|int_arrays|  squared|\n",
      "+----------+---------+\n",
      "| [1, 2, 3]|[1, 4, 9]|\n",
      "|    [4, 5]| [16, 25]|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zz = df_np.withColumn(\"squared\", spark_square_array_right(\"int_arrays\"))\n",
    "zz.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
