{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Tools Part Two\n",
    "\n",
    "### (Henri's Version)\n",
    "\n",
    "Discuss term-frequency, inverse-document-frequency, count vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"nlp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First cover tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "# pyspark.ml.feature.HashingTF(numFeatures=262144, binary=False, inputCol=None, outputCol=None) -->\n",
    "# Maps a sequence of terms to their term frequencies using the hashing trick.\n",
    "# --- binary: If True, all non zero counts are set to 1.\n",
    "#     This is useful for discrete prbabilistic models that model binary events rather than integer counts.\n",
    "#     Default False.\n",
    "# --- numFeatures: The number of features.\n",
    "\n",
    "# pyspark.ml.feature.IDF(minDocFreq=0, inputCol=None, outputCol=None) --> Compute the IDF given a collection of \n",
    "# documents.\n",
    "# --- minDocFrequency: Minimum number of documents in which a term should appear for filtering.\n",
    "\n",
    "# pyspark.ml.feature.Tokenizer(inputCol=None, outputCol=None) --> A tokenizer that converts the input string to\n",
    "# lowercase and then splits it by white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df = spark.createDataFrame(data=[\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")\n",
    "], schema=[\"label\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            sentence|\n",
      "+-----+--------------------+\n",
      "|  0.0|Hi I heard about ...|\n",
      "|  0.0|I wish java could...|\n",
      "|  1.0|Logistic regressi...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_data = tokenizer.transform(sentence_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            sentence|               words|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|Hi I heard about ...|[hi, i, heard, ab...|\n",
      "|  0.0|I wish java could...|[i, wish, java, c...|\n",
      "|  1.0|Logistic regressi...|[logistic, regres...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab the term-frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_tf = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, sentence: string, words: array<string>, rawFeatures: vector]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurized_data = hashing_tf.transform(dataset=words_data)\n",
    "featurized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+--------------------------------------------------------------------------------------+\n",
      "|words                                     |rawFeatures                                                                           |\n",
      "+------------------------------------------+--------------------------------------------------------------------------------------+\n",
      "|[hi, i, heard, about, spark]              |(262144,[24417,49304,73197,91137,234657],[1.0,1.0,1.0,1.0,1.0])                       |\n",
      "|[i, wish, java, could, use, case, classes]|(262144,[20719,24417,55551,116873,147765,162369,192310],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|[logistic, regression, models, are, neat] |(262144,[13671,91006,132713,167122,190884],[1.0,1.0,1.0,1.0,1.0])                     |\n",
      "+------------------------------------------+--------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurized_data.select([\"words\", \"rawFeatures\"]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the Inverse-Document-Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_fitted = idf.fit(dataset=featurized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled_data = idf_fitted.transform(dataset=featurized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                                                                                                                                                        |\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0.0  |(262144,[24417,49304,73197,91137,234657],[0.28768207245178085,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])                                                     |\n",
      "|0.0  |(262144,[20719,24417,55551,116873,147765,162369,192310],[0.6931471805599453,0.28768207245178085,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])|\n",
      "|1.0  |(262144,[13671,91006,132713,167122,190884],[0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])                                                    |\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescaled_data.select([\"label\", \"features\"]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```rescaled_data``` is now ready for any ML algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us now move on to CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "# pyspark.ml.feature.CountVectorizer(minTF=1.0, minDF=1.0, maxDF=9223372036854775807, vocabSize=262144, \n",
    "#                                    binary=False, inputCol=None, outputCol=None)\n",
    "# --> Extracts a vocabulary from document collections and generates a CountVectorizerModel.\n",
    "# --- binary: Binary toggle to control the output vector values.\n",
    "#             If True, all nonzero counts (after minTF filter applied) are set to 1.\n",
    "#             This is useful for discrete probabilistic models that model binary events rather than integer counts\n",
    "#             Default False.\n",
    "# --- maxDF: Specifies the maximum number of different documents a term could appear in to be included in the \n",
    "#            vocabulary.  A term that appears more than the threshold will be ignored.\n",
    "#            If this is an integer >= 1, this specifies the maximum number of documents the term could appear in.\n",
    "#            If this is a double in [0, 1), then this specifies the max fraction of documents the term could\n",
    "#            appear in.\n",
    "#            Default 2^63 - 1.\n",
    "# --- minDF: Specifies the minimum number of different documents a term must appear in to be included in the \n",
    "#            vocabulary.  If this is an integer >= 1, this specifies the number of documents the term must appear\n",
    "#            in; if this is a double in [0, 1), then this specifies the fraction of documents.\n",
    "#            Default 1.0\n",
    "# --- minTF: Filter to ignore rare words in a document. For each document, terms with frequency/count less than \n",
    "#            the given threshold are ignored.  If this is an integer >= 1, then this specifies a count (of times \n",
    "#            the term must appear in the document); if this is a double in [0,1), then this specifies a fraction \n",
    "#            (out of the document's token count). \n",
    "#            Note that the parameter is only used in transform of CountVectorizerModel and does not affect \n",
    "#            fitting. Default 1.0\")\n",
    "# --- vocabSize: Max size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data=[\n",
    "    (0, [\"a\", \"b\", \"c\"]),\n",
    "    (1, [\"a\", \"b\", \"b\", \"c\", \"a\"])\n",
    "], schema=[\"id\", \"words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "| id|          words|\n",
      "+---+---------------+\n",
      "|  0|      [a, b, c]|\n",
      "|  1|[a, b, b, c, a]|\n",
      "+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_fitted = count_vect.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cv_fitted.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------------------+\n",
      "|id |words          |features                 |\n",
      "+---+---------------+-------------------------+\n",
      "|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      "|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      "+---+---------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(truncate=False)\n",
    "# Essentially the bag-of-words method that we discussed earlier on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
