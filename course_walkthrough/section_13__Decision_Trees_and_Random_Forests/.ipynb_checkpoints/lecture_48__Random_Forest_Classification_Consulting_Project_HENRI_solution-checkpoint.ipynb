{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classification Consulting Project\n",
    "\n",
    "## (Henri's personal Solution + Additional Comments)\n",
    "\n",
    "You have been contracted by the Purina Dog Food Company and flown out to their HQ in St Louis, Missouri.\n",
    "\n",
    "You've been hired by a dog food company to try predict why some batches of their dog food are spoiling much quicker than intended.\n",
    "\n",
    "Unfortunately, this Dog Food Comapny hasn't upgraded to the latest machinery, meaning that the amounts of the five preservative chemicals they are using can vary a lot, but which is the chemical that has the strongest effect?\n",
    "\n",
    "The dog food company first mixes up a batch of preservative that contains 4 different preservative chemical (A, B, C, D) and then is completed with a \"filler\" chemical.\n",
    "\n",
    "The food scientists believe one of the A, B, C, or D preservatives is causing the prolem, but need your help to figure out which one.\n",
    "\n",
    "Use Machine Learning with RF (Random Forests) to find out which parameter had the most predictive power, thus finding out which chemical causes the early spoiling!\n",
    "\n",
    "So create a model and then find out how you can decide which chemical is the problem!\n",
    "\n",
    "Their data looks like this:\n",
    "- ```Pres_A```:  Percentage of preservative A in the mix.\n",
    "- ```Pres_B```:  Percentage of preservative B in the mix.\n",
    "- ```Pres_C```:  Percentage of preservative C in the mix.\n",
    "- ```Pres_D```:  Percentage of preservative D in the mix.\n",
    "- ```Spoiled```: Label indicating whether or not the dog food batch was spoiled.\n",
    "\n",
    "Think carefully about what this problem is really asking you to solve.\n",
    "\n",
    "While we will use Machine Learning to solve this, it won't be your typical train/test split workflow.  If this confuses you, skip ahead to the solution code-along."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some additional comments from the upcoming code-along solution video presentation:\n",
    "\n",
    "Many machine learning models produce some sort of coefficient value for each feature involved, indicating their \"importance\" or predictive power.\n",
    "\n",
    "We mentioned beforehand that tree method classifiers have a ```.featureImportances``` attribute available.\n",
    "\n",
    "So we can create a model, fit it on all the data, and then check which feature (preservative) was causing the spoilage.\n",
    "\n",
    "```.featureImportances``` returns:\n",
    "- ```SparseVector(4, {0: 0.031, 1: 0.026, 2: 0.912, 3: 0.031})```\n",
    "- Corresponding to a **features column**:\n",
    "- ```Row(features=DenseVector([4.0, 2.0, 12.0, 3.0]), Spoiled=1.0)```\n",
    "\n",
    "There are many different ways to solve this problem, including just using \"pure\" statistics instead of a machine learning model.\n",
    "\n",
    "Hopefully this consulting project shows how we can apply machine learning in a different way from previous examples.\n",
    "\n",
    "In this case, **we con't really care about train/test splits or deployments.**\n",
    "\n",
    "What we really want to understand is the fundamental relationship between each feature column and the label itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a Spark Session:\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"dogfood\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"dog_food.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- A: integer (nullable = true)\n",
      " |-- B: integer (nullable = true)\n",
      " |-- C: double (nullable = true)\n",
      " |-- D: integer (nullable = true)\n",
      " |-- Spoiled: double (nullable = true)\n",
      "\n",
      "\n",
      "\n",
      "Row(A=4, B=2, C=12.0, D=3, Spoiled=1.0)\n",
      "Row(A=5, B=6, C=12.0, D=7, Spoiled=1.0)\n",
      "Row(A=6, B=2, C=13.0, D=6, Spoiled=1.0)\n",
      "\n",
      "\n",
      "+-------+------------------+------------------+------------------+------------------+-------------------+\n",
      "|summary|                 A|                 B|                 C|                 D|            Spoiled|\n",
      "+-------+------------------+------------------+------------------+------------------+-------------------+\n",
      "|  count|               490|               490|               490|               490|                490|\n",
      "|   mean|  5.53469387755102| 5.504081632653061| 9.126530612244897| 5.579591836734694| 0.2857142857142857|\n",
      "| stddev|2.9515204234399057|2.8537966089662063|2.0555451971054275|2.8548369309982857|0.45221563164613465|\n",
      "|    min|                 1|                 1|               5.0|                 1|                0.0|\n",
      "|    max|                10|                10|              14.0|                10|                1.0|\n",
      "+-------+------------------+------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some EDA\n",
    "data.printSchema()\n",
    "print(\"\\n\")\n",
    "\n",
    "for row in data.head(3):\n",
    "    print(row)\n",
    "print(\"\\n\")\n",
    "\n",
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proceed by formatting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'C', 'D', 'Spoiled']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['A', 'B', 'C', 'D'],\n",
    "                            outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_assembled = assembler.transform(dataset=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- A: integer (nullable = true)\n",
      " |-- B: integer (nullable = true)\n",
      " |-- C: double (nullable = true)\n",
      " |-- D: integer (nullable = true)\n",
      " |-- Spoiled: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_assembled.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|           features|Spoiled|\n",
      "+-------------------+-------+\n",
      "| [4.0,2.0,12.0,3.0]|    1.0|\n",
      "| [5.0,6.0,12.0,7.0]|    1.0|\n",
      "| [6.0,2.0,13.0,6.0]|    1.0|\n",
      "| [4.0,2.0,12.0,1.0]|    1.0|\n",
      "| [4.0,2.0,12.0,3.0]|    1.0|\n",
      "|[10.0,3.0,13.0,9.0]|    1.0|\n",
      "| [8.0,5.0,14.0,5.0]|    1.0|\n",
      "| [5.0,8.0,12.0,8.0]|    1.0|\n",
      "| [6.0,5.0,12.0,9.0]|    1.0|\n",
      "| [3.0,3.0,12.0,1.0]|    1.0|\n",
      "| [9.0,8.0,11.0,3.0]|    1.0|\n",
      "|[1.0,10.0,12.0,3.0]|    1.0|\n",
      "|[1.0,5.0,13.0,10.0]|    1.0|\n",
      "|[2.0,10.0,12.0,6.0]|    1.0|\n",
      "|[1.0,10.0,11.0,4.0]|    1.0|\n",
      "| [5.0,3.0,12.0,2.0]|    1.0|\n",
      "| [4.0,9.0,11.0,8.0]|    1.0|\n",
      "| [5.0,1.0,11.0,1.0]|    1.0|\n",
      "|[4.0,9.0,12.0,10.0]|    1.0|\n",
      "| [5.0,8.0,10.0,9.0]|    1.0|\n",
      "+-------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_assembled.select([\"features\", \"Spoiled\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform random-forest classification on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(labelCol=\"Spoiled\", maxDepth=10, numTrees=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the RandomForestClassifier.\n",
    "rfc_fitted = rfc.fit(data_assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model predictions.\n",
    "rfc_preds = rfc_fitted.transform(data_assembled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For interest sake, let us see how well our model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_eval = MulticlassClassificationEvaluator(labelCol=\"Spoiled\", metricName=\"f1\")\n",
    "prec_eval = MulticlassClassificationEvaluator(labelCol=\"Spoiled\", metricName=\"weightedPrecision\")\n",
    "rec_eval = MulticlassClassificationEvaluator(labelCol=\"Spoiled\", metricName=\"weightedRecall\")\n",
    "acc_eval = MulticlassClassificationEvaluator(labelCol=\"Spoiled\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.9959.\n",
      "Precision: 0.9959.\n",
      "Recall: 0.9959.\n",
      "Accuracy: 0.9959.\n"
     ]
    }
   ],
   "source": [
    "print(\"F1-score: {:6.4f}.\".format(f1_eval.evaluate(dataset=rfc_preds)))\n",
    "print(\"Precision: {:6.4f}.\".format(prec_eval.evaluate(dataset=rfc_preds)))\n",
    "print(\"Recall: {:6.4f}.\".format(rec_eval.evaluate(dataset=rfc_preds)))\n",
    "print(\"Accuracy: {:6.4f}.\".format(acc_eval.evaluate(dataset=rfc_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For interest sake, see what is the ```auroc```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc_eval = BinaryClassificationEvaluator(labelCol=\"Spoiled\", metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC = 0.9998.\n"
     ]
    }
   ],
   "source": [
    "print(\"AUROC = {:6.4f}.\".format(auroc_eval.evaluate(dataset=rfc_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess the feature importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index = 0.  Perc = 0.031.\n",
      "Index = 1.  Perc = 0.026.\n",
      "Index = 2.  Perc = 0.912.\n",
      "Index = 3.  Perc = 0.031.\n"
     ]
    }
   ],
   "source": [
    "# Estimate the importance of each feature:\n",
    "for (idx, frac) in enumerate(rfc_fitted.featureImportances):\n",
    "    print(\"Index = {:.0f}.  Perc = {:.3f}.\".format(idx, frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Index 2 corresponds with label C.***\n",
    "\n",
    "***The feature with the most predictive power is ```Pres_C```.***\n",
    "\n",
    "***Preservative C is causing the spoiling.***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
