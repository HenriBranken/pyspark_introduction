{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming Documentation Example\n",
    "\n",
    "The interested reader can have a look at how **Structured Streaming** is performed at:  \n",
    "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#quick-example\n",
    "\n",
    "In this Notebook, we will work through good old-fashioned Spark Streaming, which is detailed at:  \n",
    "https://spark.apache.org/docs/latest/streaming-programming-guide.html#a-quick-example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we will be using Spark Streaming and not structured streaming, we need to use some older \"RDD\" syntax.\n",
    "\n",
    "This stems from using a SparkContext instead of a SparkSession.\n",
    "\n",
    "We will be building a very simple application that connects to a local stream of data (an open terminal) through a socket connection.\n",
    "\n",
    "It will then count the words for each line that we type in.\n",
    "\n",
    "The steps for streaming will be:\n",
    "- Create a SparkContext.\n",
    "- Create a StreamingContext.\n",
    "- Create a Socket Text Stream.\n",
    "- Read in the lines as a \"DStream\".\n",
    "\n",
    "The steps for working with the Data:\n",
    "- Split the input line into a list of words.\n",
    "- Map each work to a tuple: ```(word, 1)```.  The second parameter will always be 1.\n",
    "- Then group (**reduce**) the tuples by the work (**key**), and sum up the second argument (the number one).  I.e., we _reduce by key_.\n",
    "\n",
    "That will then provide us with a word count in the form **('hello', 3)** for each line.\n",
    "\n",
    "As a quick note, the RDD syntax relies heavily on lambda expressions, which are just quick anonymous functions.\n",
    "\n",
    "Fortunately, all the lambda expressions used here are quite simple and basic.\n",
    "\n",
    "Let's get started with this simple example!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Example\n",
    "\n",
    "First, we import ```StreamingContext```, which is the main entry point for all streaming functionality.  We create a local StreamingContext with 2 execution threads, and a batch interval of 1 second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local StreamingContext with 2 working threads and a batch interval of 1 second.\n",
    "sc = SparkContext(\"local[2]\", \"NetworkWordCount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 1)  # This means that the interval is 1 second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this context, we can create a DStream that represents streaming data from a TCP source, specified as hostname (e.g. ```localhost```) and port (e.g. ```9999```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)  # use a port that we are pretty darn sure is not being \n",
    "# currently used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```lines``` DStream represents the stream of Data that will be received from the data server.  Each record in this DStream is a line of text.  Next, we want to split the lines by space into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each line into words.\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))  # Allows to map something to those lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```flatMap``` is a one-to-many DStream operation that creates a new DStream by generating multiple new records from each record in the source DStream.  In this case, each line will be split into multiple words and the stream of words is represented as the ```words``` DStream.  Next, we want to count these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda num1, num2: num1 + num2)  # think of it as a groupBy mechanism.\n",
    "# Keep iteratively reducing untill you are only left with one tuple instance of the word.\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console.\n",
    "wordCounts.pprint()  # pretty print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```words``` DStream is further mapped (one-to-one transformation) to a DStream of ```(word, 1)``` pairs, which is then reduced to get the frequency of words in each batch of data.  Finally ```wordCounts.pprint()``` will print a few of the counts generated every second.\n",
    "\n",
    "Note that when these lines are executed, Spark Streaming only sets up the computation it will perform when it is started, and no real processing has started yet.  To start the processing after all the transformation have been setup, we finally call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-06-06 14:54:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-06-06 14:54:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-06-06 14:54:07\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-06-06 14:54:08\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-06-06 14:54:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-06-06 14:54:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-06-06 14:54:11\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-06-06 14:54:12\n",
      "-------------------------------------------\n",
      "('now', 1)\n",
      "('how', 1)\n",
      "('about', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-06-06 14:54:13\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()  # Start the computation\n",
    "\n",
    "# If you have already downloaded and built Spark, you can run this example as follows.  \n",
    "# You will first need to run NetCat (a small utility found in most Unix-like systems) as a data server by using:  \n",
    "# $ nc -lk 9999\n",
    "\n",
    "# Use the STOP button above to stop the streaming.\n",
    "# This might reflect as a KeyboardInterruptError in the output of this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete code can be found in the Spark Streaming example https://github.com/apache/spark/blob/v2.4.3/examples/src/main/python/streaming/network_wordcount.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
